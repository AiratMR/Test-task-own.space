{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorymembers(categorymembers, titles, level=0, max_level=1):\n",
    "    \"\"\"Возращает заголовки статей википедии из запрашиваемой категории\n",
    "    \n",
    "    Аргументы:\n",
    "    categorymembers -- список с членами запрашиваемой категории\n",
    "    titles -- список, в который будут добавляться заголовки статей\n",
    "    \n",
    "    Именные аргументы:\n",
    "    level -- integer - нижний уровень вложенности категорий и подкатегорий\n",
    "    max_level -- integer - нижний уровень вложенности категорий и подкатегорий\n",
    "    \n",
    "    \"\"\"\n",
    "    for c in categorymembers.values():\n",
    "        titles.append(c.title)\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "            get_categorymembers(c.categorymembers, titles,level=level + 1, max_level=max_level)\n",
    "\n",
    "    return(titles)\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "# Категория - медицина\n",
    "cat = wiki_wiki.page(\"Category:Medicine\") \n",
    "titles = []\n",
    "titles = get_categorymembers(cat.categorymembers, titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Категория - наука, так как научные статьи более схожы с медицинскими и, я думаю,\n",
    "# это хорошо отразится на качестве модели\n",
    "random_cat = wiki_wiki.page(\"Category:Science\") \n",
    "random_titles = []\n",
    "random_titles = get_categorymembers(random_cat.categorymembers, random_titles)\n",
    "\n",
    "#p_wiki = wiki_wiki.page(titles[0])\n",
    "#print(p_wiki.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarticles(titles):\n",
    "    \"\"\"Возвращает список статей\n",
    "    \n",
    "    Аргументы:\n",
    "    titles - список заголовков искомых статей\n",
    "    \n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    for i in range(2500):\n",
    "        try:\n",
    "            p_wiki = wiki_wiki.page(titles[i])\n",
    "            articles.append(p_wiki.text)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "class_1 = getarticles(titles)\n",
    "print(len(class_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2232\n"
     ]
    }
   ],
   "source": [
    "class_2 = getarticles(random_titles)\n",
    "print(len(class_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Возращает предобработанный текст\n",
    "    \n",
    "    Этапы обработки:\n",
    "    1. Привидение к нижнему регистру;\n",
    "    2. Замена ссылок ключевым словом URL;\n",
    "    3. Замена знаков препинания и других символов, не относящимся к буквам или словам, пробелами;\n",
    "    4. Замена множества пробелов одним пробелом.\n",
    "    \n",
    "    Предобработка нужна для очистки текста от ненужных символов, которые не участвуют в классификации.\n",
    "    \n",
    "    Аргументы:\n",
    "    text -- строка с текстом\n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я]+', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = class_1[:1400] + class_2[:1500]\n",
    "data = [preprocess_text(t) for t in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def delete_stopwords(text):\n",
    "    \"\"\"Возвращает текст без стоп-слов\n",
    "    \n",
    "    К стоп-словам относятся слова не несущие никакой смысловой нагрузки, такие как:\n",
    "    артикли, союзы, предлоги и т.д. Такие слова делают классификацию менее точной, так как\n",
    "    их, как правило, больше всего, и модель может ошибочно сделать их значимыми.\n",
    "    \n",
    "    Аргументы:\n",
    "    text -- строка с текстом\n",
    "    \n",
    "    \"\"\"\n",
    "    text = text.split(' ')\n",
    "    filtered_text = [word for word in text if word not in stopwords.words('english')]\n",
    "    res_text = ''\n",
    "    for word in filtered_text:\n",
    "        res_text += word + ' '\n",
    "    return res_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = [delete_stopwords(t) for t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Максимальное количество слов в предложении\n",
    "SENTENCE_LENGTH = 150 \n",
    "# Размер словаря\n",
    "NUM = 100000\n",
    "\n",
    "def get_sequences(tokenizer, x):\n",
    "    \"\"\"Возращает массив идентификаторов токенов\n",
    "    \n",
    "    Токенизация нужна для того, что текст представить в виде чисел\n",
    "    \n",
    "    Аргументы:\n",
    "    tokenizer -- токенизатор\n",
    "    x -- массив с текстами\n",
    "    \n",
    "    \"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
    "\n",
    "# Cоздаем и обучаем токенизатор\n",
    "tokenizer = Tokenizer(num_words=NUM)\n",
    "tokenizer.fit_on_texts(filtered_data)\n",
    "\n",
    "# Отображаем каждый текст в массив идентификаторов токенов\n",
    "x_train = get_sequences(tokenizer, filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0    12 16650   231   884  3317\n",
      "   914 16241   219     5    26   114  1729    45   110    34 16650  2006\n",
      "  3078   914    13   219    26  2454 10449 10449  4290  1829 11806   667\n",
      "   470    61   174 14122  2536   990   213  9735   164 62466    94   159\n",
      " 17814   333  3857   686  4143   333  6242  1370   159 16727  2569  2371\n",
      "  2267  1183 10449 10449  1829    34   284   164  1386    26  1310     6\n",
      " 27433  4070 19930  1109    61  2323  1260  1494  1364  3355   177  2171\n",
      "   164     6 27433   675  5115  1000   780   231  3689  1183 10449     6\n",
      "     4   528 14310   477    79 10880    48     4   830 26961   732 14754\n",
      "   148  1066   362  3078    12    54]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# массив с праильными ответами для обучения с учителем\n",
    "y_train = []\n",
    "\n",
    "for i in range(2900):\n",
    "    if i < 1400:\n",
    "        y_train.append(1)\n",
    "    else:\n",
    "        y_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#print(x_train.shape)\n",
    "#print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.loadtxt(\"x_train.txt\")\n",
    "y_train = np.loadtxt(\"y_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"x_train.txt\", x_train)\n",
    "np.savetxt(\"y_train.txt\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(X_train)\n",
    "std = np.std(X_train)\n",
    "X_train = (X_train - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Слой Embedding для векторного представления слов\n",
    "# Слой располагает вектора слов со схожим знаением на близком расстоянии в процессе обучения\n",
    "model.add(Embedding(100000, 64))\n",
    "# Слой LSTM - долгой краткосрочной памяти, параметры дропаута и количества запоминающих элементов выбраны интуитивно\n",
    "model.add(LSTM(100, dropout=0.25, recurrent_dropout=0.25))\n",
    "# Выходной слой с одним нейроном с сигмоидной функцией активации для бинарной классификации\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2175 samples, validate on 725 samples\n",
      "Epoch 1/10\n",
      "2175/2175 [==============================] - 26s 12ms/sample - loss: 0.6624 - acc: 0.6395 - val_loss: 0.9142 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2175/2175 [==============================] - 23s 11ms/sample - loss: 0.5642 - acc: 0.6846 - val_loss: 1.1010 - val_acc: 0.3545\n",
      "Epoch 3/10\n",
      "2175/2175 [==============================] - 23s 11ms/sample - loss: 0.3038 - acc: 0.8740 - val_loss: 0.9841 - val_acc: 0.5738\n",
      "Epoch 4/10\n",
      "2175/2175 [==============================] - 23s 11ms/sample - loss: 0.1748 - acc: 0.9269 - val_loss: 1.0583 - val_acc: 0.5586\n",
      "Epoch 5/10\n",
      "2175/2175 [==============================] - 23s 11ms/sample - loss: 0.1592 - acc: 0.9163 - val_loss: 1.0315 - val_acc: 0.5614\n",
      "Epoch 6/10\n",
      "2175/2175 [==============================] - 23s 11ms/sample - loss: 0.1402 - acc: 0.9292 - val_loss: 1.1703 - val_acc: 0.5103\n",
      "Epoch 7/10\n",
      "2175/2175 [==============================] - 23s 11ms/sample - loss: 0.1296 - acc: 0.9264 - val_loss: 1.3763 - val_acc: 0.5379\n",
      "Epoch 8/10\n",
      "2175/2175 [==============================] - 22s 10ms/sample - loss: 0.1198 - acc: 0.9292 - val_loss: 1.7168 - val_acc: 0.4772\n",
      "Epoch 9/10\n",
      "2175/2175 [==============================] - 22s 10ms/sample - loss: 0.1177 - acc: 0.9306 - val_loss: 1.9200 - val_acc: 0.5779\n",
      "Epoch 10/10\n",
      "2175/2175 [==============================] - 22s 10ms/sample - loss: 0.1224 - acc: 0.9333 - val_loss: 1.5526 - val_acc: 0.4814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x218d91f7358>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=100, epochs=10, validation_split=0.25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1415   388  2840 14248  3223  2510  5704  6427    83   896 26012   440\n",
      "   5621 10768   750  3036 14260   561  9541   931   649  5621   527  4017\n",
      "   2499   779   164    73 26012   649  5006  5438   337  2845   209   178\n",
      "    440   337  3706 18168  1545  7283  9821    53    32   116   248     4\n",
      "    203  1495 26012   440  6951   337   297   139    69  7283  9821   630\n",
      "    528  4962   209  2616  6951   209    69  1127  3109  7283  9821   630\n",
      "    528  4962  6951   209  1750   630  1366  3737  4962  3706  4962 10553\n",
      "   5005   597    98 26012    47  1727  4940  7204    22  3031   890  5704\n",
      "   5704 39581   356   114 14781  1019 15740   715 17326 14194  3886  2415\n",
      "     47   829  4326 11294   971  4940  3223 14781  1019 25814 11106  4380\n",
      "  11892  1765 10528  7969  2598  3456  2415   211  1202  6949  4940  3223\n",
      "     47   829  1127  5704   971  3223   297   231  1808    83   896   263\n",
      "    763  8362 11294   395    22   890]]\n",
      "[[0.9833844]]\n"
     ]
    }
   ],
   "source": [
    "text = \"Coronary artery bypass surgery, also known as coronary artery bypass graft (CABG, pronounced \\\"cabbage\\\") surgery, and colloquially heart bypass or bypass surgery, is a surgical procedure to restore normal blood flow to an obstructed coronary artery. A normal coronary artery transports blood to and from the heart muscle itself, not through the main circulatory system. There are two main approaches. In one, the left internal thoracic artery, LITA (also called left internal mammary artery, LIMA) is diverted to the left anterior descending branch of the left coronary artery. In this method, the artery is \\\"pedicled\\\" which means it is not detached from the origin. In the other, a great saphenous vein is removed from a leg; one end is attached to the aorta or one of its major branches, and the other end is attached to the obstructed artery immediately after the obstruction to restore blood flow. CABG is performed to relieve angina unsatisfactorily controlled by maximum tolerated anti-ischemic medication, prevent or relieve left ventricular dysfunction, and/or reduce the risk of death. CABG does not prevent myocardial infarction (heart attack). This surgery is usually performed with the heart stopped, necessitating the usage of cardiopulmonary bypass. However, two alternative techniques are also available, allowing CABG to be performed on a beating heart either without using the cardiopulmonary bypass, a procedure referred to as \\\"off-pump\\\" surgery, or performing beating surgery using partial assistance of the cardiopulmonary bypass, a procedure referred to as \\\"on-pump beating\\\" surgery. The latter procedure offers the advantages of the on-pump stopped and off-pump while minimizing their respective side-effects. CABG is often indicated when coronary arteries have a 50 to 99 percent obstruction. The obstruction being bypassed is typically due to arteriosclerosis, atherosclerosis, or both. Arteriosclerosis is characterized by thickening, loss of elasticity, and calcification of the arterial wall, most often resulting in a generalized narrowing in the affected coronary artery. Atherosclerosis is characterized by yellowish plaques of cholesterol, lipids, and cellular debris deposited into the inner layer of the wall of a large or medium-sized coronary artery, most often resulting in a partial obstruction in the affected artery. Either condition can limit blood flow if it causes a cross-sectional narrowing of at least 50 percent. \"\n",
    "\n",
    "#text = \"The Persian cat is a long-haired breed of cat characterized by its round face and short muzzle. It is also known as the \\\"Persian Longhair\\\" in the English-speaking countries. In the Middle East region they are widely known as \\\"Iranian cat\\\" and in Iran they are known as \\\"Shirazi cat\\\". The first documented ancestors of the Persian were imported into Italy from Iran (historically known as Persia) around 1620. The exact history of the Persian cat does seem to be a bit of a mystery but many of these long-haired cats were seen in hieroglyphics. The story has it that these long-haired cats were then imported into Europe as their popularity grew and breeding took place in Italy and France.\"\n",
    "#text = \"A chondroblastoma is a rare, usually benign, tumor of bone that accounts for approximately 1% of all bone tumors. In 1931, Codman classified it as a chondromatous variant of giant cell tumors, when he described these lesions in the proximal humerus. [1] A decade later, Jaffe and Lichtenstein renamed the Codman tumor a benign chondroblastoma to emphasize the chondroblastic genesis of the lesion and to distinguish it from the classic giant cell tumor of bone. [2]Although the exact etiology of chondroblastoma remains uncertain, the presentation, appropriate evaluation, and treatment of patients with the condition have been well described. (See Presentation, Workup, and Treatment.)\"\n",
    "\n",
    "#text = \"The R-boats built by Lake Torpedo Boat Company (R-21 through R-27) are sometimes considered a separate class from those of the other builders. The Lake boats had a length of 175 feet (53.3 m) overall, a beam of 16 feet 8 inches (5.1 m) and a mean draft of 13 feet 11 inches (4.2 m). They displaced 497 long tons (505 t) on the surface and 652 long tons (662 t) submerged. The R-class submarines had a crew of 3 officers and 23 enlisted men. They had a diving depth of 200 feet (61.0 m).[1] For surface running, the boats were powered by two 500-brake-horsepower (373 kW) diesel engines, each driving one propeller shaft. When submerged each propeller was driven by a 400-horsepower (298 kW) electric motor. They could reach 14 knots (26 km/h; 16 mph) on the surface and 11 knots (20 km/h; 13 mph) underwater. On the surface, the Lake boats had a range of 3,523 nautical miles (6,525 km; 4,054 mi) at 11 knots (20 km/h; 13 mph)[1] and 150 nmi (280 km; 170 mi) at 5 knots (9.3 km/h; 5.8 mph) submerged.[1] The boats were armed with four 21-inch (53.3 cm) torpedo tubes in the bow. They carried four reloads, for a total of eight torpedoes. The R-class submarines were also armed with a single 3\\\"/50 caliber deck gun.[2]\"\n",
    "\n",
    "text = preprocess_text(text)\n",
    "text = delete_stopwords(text)\n",
    "text = get_sequences(tokenizer, [text])\n",
    "print(text)\n",
    "y_p = model.predict_proba(text)\n",
    "print(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Сохранение токенизатора\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Сохранение модели в формате json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Сохранение весов в формате hdf5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
